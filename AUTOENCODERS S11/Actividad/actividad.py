# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11ZT5APtju1aMv6cLF75iP8jaSLsl8zdv
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# Generamos datos sintéticos simulando tráfico normal y algunas anomalías
np.random.seed(42)
normal_data = np.random.normal(loc=0, scale=1, size=(1000, 5)) # 1000 muestras normales con 5 características
anomalous_data = np.random.normal(loc=5, scale=1, size=(50, 5)) # 50 muestras anómalas
# Concatenamos los datos
X = np.vstack([normal_data, anomalous_data])
labels = np.array([0] * 1000 + [1] * 50) # 0 = Normal, 1 = Anomalía
# Normalizamos los datos
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
# Dividimos en conjunto de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X_scaled, labels, test_size=0.2,random_state=42)
# Definimos la arquitectura del Autoencoder
input_dim = X_train.shape[1]
encoding_dim = 2 # Comprimimos los datos a 2 dimensiones
input_layer = keras.layers.Input(shape=(input_dim,))
encoded = keras.layers.Dense(encoding_dim, activation='relu')(input_layer)
decoded = keras.layers.Dense(input_dim, activation='sigmoid')(encoded)
autoencoder = keras.models.Model(input_layer, decoded)
autoencoder.compile(optimizer='adam', loss='mse')
# Entrenamos el Autoencoder solo con datos normales
autoencoder.fit(X_train[y_train == 0], X_train[y_train == 0],
epochs=50, batch_size=16, shuffle=True, validation_data=(X_test[y_test == 0], X_test[y_test == 0]))

# Calculamos el error de reconstrucción para el conjunto de prueba
X_test_pred = autoencoder.predict(X_test)
reconstruction_error = np.mean(np.abs(X_test - X_test_pred), axis=1)
# Establecemos un umbral para detectar anomalías
threshold = np.percentile(reconstruction_error, 95)
anomaly_predictions = (reconstruction_error > threshold).astype(int)
# Visualizamos los resultados
plt.hist(reconstruction_error, bins=50, alpha=0.7, label='Error de Reconstrucción')
plt.axvline(threshold, color='r', linestyle='dashed', linewidth=2, label='Umbral de Anomalía')
plt.xlabel('Error de Reconstrucción')
plt.ylabel('Frecuencia')
plt.title('Distribución del Error de Reconstrucción')
plt.legend()
plt.show()

# Evaluamos la detección de anomalías
from sklearn.metrics import classification_report
print(classification_report(y_test, anomaly_predictions))